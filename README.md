# Hallucination-aware Optimization for Large Language Model-empowered Communications

## Links:

TeleQnA: https://huggingface.co/datasets/netop/TeleQnA

Falcon-series LLMs: https://huggingface.co/blog/falcon#the-falcon-models

## Dataset:
Available at train.json (training set) and test.json (test set)


## References of LLM-empowered Communications:
A1: Y. Du et al., “The power of large language models for wireless communication system development: A case study on FPGA platforms,” arXiv: 2307.07319, 2023

A2: L. Bariah et al., “Understanding telecom language through large language models,” arXiv:2306.07933, 2023

A3: E. Aghaei et al., “Securebert: A domain-specific Language Model for Cybersecurity,” in Proc. Intl. Conf. Security and Privacy in Communication Systems, 2022, pp. 39–56

A4: J. Tong et al., “WirelessAgent: Large language model agents for intelligent wireless networks,” arXiv: 2409.07964, 2024

A5: Y. Shen et al., "Large language models empowered autonomous edge AI for connected intelligence," in IEEE Communications Magazine, vol. 62, no. 10, pp. 140-146, 2024

A6: W. Lee et al., "LLM-empowered resource allocation in wireless communications systems," arXiv: 2408.02944v1, 2024

A7: G.M. Yilma et al., "TelecomRAG: Taming Telecom standards with retrieval augmented generation and LLMs," arXiv: 2406.07053, 2024

A8:  H. Zou et al., “Wireless multi-agent generative AI: From connected intelligence to collective intelligence,” arXiv preprint arXiv:2307.02757, 2024.
